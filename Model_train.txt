import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
from sklearn.preprocessing import LabelEncoder

class CommandResponseDataset(Dataset):
    def __init__(self, csv_file):
        self.data = pd.read_csv(csv_file)
        self.commands = self.data['command'].values
        self.responses = self.data['response'].values
        
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(self.responses)
        
        self.vocab = set()
        for command in self.commands:
            self.vocab.update(command.split())
        self.vocab = {word: idx for idx, word in enumerate(self.vocab, 1)}
        self.vocab['<PAD>'] = 0

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        command = self.commands[idx].split()
        command_encoded = [self.vocab[word] for word in command]
        response_encoded = self.label_encoder.transform([self.responses[idx]])[0]
        return torch.tensor(command_encoded), torch.tensor(response_encoded)
    
    def pad_collate_fn(self, batch):
        commands, responses = zip(*batch)
        commands_padded = nn.utils.rnn.pad_sequence(commands, batch_first=True, padding_value=self.vocab['<PAD>'])
        responses = torch.tensor(responses)
        return commands_padded, responses

class ResidualBlock(nn.Module):
    def __init__(self, hidden_dim):
        super(ResidualBlock, self).__init__()
        self.layer1 = nn.Linear(hidden_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_dim, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        identity = x
        out = self.layer1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.layer2(out)
        out = self.bn2(out)
        out = self.dropout(out)
        out += identity
        out = self.relu(out)
        return out

class DeepLLMModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=500):
        super(DeepLLMModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.input_layer = nn.Linear(embed_dim, hidden_dim)
        self.deep_layers = nn.Sequential(
            *[ResidualBlock(hidden_dim) for _ in range(num_layers)]
        )
        self.output_layer = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        x = self.embedding(x).mean(dim=1)  # Average the embeddings
        x = self.input_layer(x)
        x = self.deep_layers(x)
        out = self.output_layer(x)
        return out

def calculate_accuracy(outputs, targets):
    _, predictions = torch.max(outputs, dim=1)
    correct = (predictions == targets).sum().item()
    return correct / len(targets)

def train_model(model, train_loader, criterion, optimizer, epochs=25, save_path='deep_llm_model.pth'):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        running_accuracy = 0.0
        for commands, responses in train_loader:
            optimizer.zero_grad()
            outputs = model(commands)
            loss = criterion(outputs, responses)
            
            if torch.isnan(loss):
                print("NaN loss encountered, skipping this batch")
                continue
            
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            running_accuracy += calculate_accuracy(outputs, responses)
        
        epoch_loss = running_loss / len(train_loader)
        epoch_accuracy = running_accuracy / len(train_loader)
        print(f"Epoch {epoch+1}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}")
    
    # Save the model after training
    torch.save(model.state_dict(), save_path)
    print(f"Model saved to {save_path}")

def load_model(model_class, vocab_size, embed_dim, hidden_dim, output_dim, num_layers, load_path):
    model = model_class(vocab_size, embed_dim, hidden_dim, output_dim, num_layers)
    model.load_state_dict(torch.load(load_path))
    model.eval()  # Set the model to evaluation mode
    return model

def generate_response(model, command, vocab, label_encoder, max_length=50):
    words = command.split()
    command_encoded = [vocab.get(word, vocab['<PAD>']) for word in words]
    command_tensor = torch.tensor([command_encoded])
    
    with torch.no_grad():
        output = model(command_tensor)
        predicted_response_idx = output.argmax(dim=1).item()
        predicted_response = label_encoder.inverse_transform([predicted_response_idx])[0]
    return predicted_response

def main():
    # Load and preprocess data
    dataset = CommandResponseDataset('dataset.csv')
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=dataset.pad_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=dataset.pad_collate_fn)
    
    # Define model parameters
    vocab_size = len(dataset.vocab)
    embed_dim = 128
    hidden_dim = 256
    output_dim = len(dataset.label_encoder.classes_)
    
    # Initialize model, loss function, and optimizer
    model = DeepLLMModel(vocab_size, embed_dim, hidden_dim, output_dim, num_layers=1)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Train the model and save it
    train_model(model, train_loader, criterion, optimizer, epochs=25, save_path='deep_llm_model.pth')
    
    # Load the model and generate a response
    model = load_model(DeepLLMModel, vocab_size, embed_dim, hidden_dim, output_dim, 1, 'deep_llm_model.pth')
    test_command = input("Enter the command:")
    response = generate_response(model, test_command, dataset.vocab, dataset.label_encoder)
    print(f"Response: {response}")

if __name__ == '__main__':
    main()
